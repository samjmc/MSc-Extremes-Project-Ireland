---
title: "Storm Surge Data"
author: "Samuel McDonnell"
date: "2024-03-13"
output: html_document
---

```{r}
data <- readRDS("gtsr_daily_max_1979_2014.rds")
```

```{r}
head(data)
names(data)
levels(data$station_name)
```

```{r}

set.seed(123)
#MAPPING THE LON AND LAT POINTS
library(sf)
library(ggplot2)
library(dplyr)
library(tmap)
library(tidyr)

#Load spatial data
ireland <- st_read("IRL_adm0.shp")

#UNIQUE COORDS FOR LAT AND LON
unique_coords <- unique(data[, c("lon", "lat")])

#CONVERT TO Sf object
unique_coords_sf <- st_as_sf(unique_coords, coords = c("lon", "lat"), crs = 4326)

#PLOT
tm_shape(ireland) +
  tm_borders() +
  tm_shape(unique_coords_sf) +
  tm_dots(col = "blue", size = 0.05, title = "Unique Coordinates") +
  tm_style("gray")



```

```{r}
dun_laoghaire<- subset(data, station_name == "795") #795 dun laoghaire

# Convert the selected point to an sf object
dun_laoghaire_sf <- st_as_sf(dun_laoghaire, coords = c("lon", "lat"), crs = 4326)

# Plot the map
tm_shape(ireland) +  
  tm_borders() +      
  tm_shape(dun_laoghaire_sf) +  
  tm_dots(col = "blue", size = 0.5, title = "dun_laoghaire Observation") + 
  tm_style("gray")   



```

```{r}
# Time series plot
ggplot(dun_laoghaire, aes(x = as.Date(date_time), y = surge_daily_max)) +
  geom_line() +
  labs(title = "Sea Level Surge Daily Maximum Values Over Time",
       x = "Date", y = "Surge Daily Max")
```

```{r}
# Seasonal decomposition additive
ts_data <- ts(dun_laoghaire$surge_daily_max, frequency = 365.25)
ts_decomp <- decompose(ts_data)
plot(ts_decomp)

#multiplicative
ts_data2 <- ts(dun_laoghaire$surge_daily_max, frequency = 365.25)
ts_decomp2 <- decompose(ts_data2, type = "multiplicative")
plot(ts_decomp2)
```

```{r}
# Define custom ordering of months from July to June
custom_order <- c("Jul", "Aug", "Sep", "Oct", "Nov", "Dec", "Jan", "Feb", "Mar", "Apr", "May", "Jun")

# Convert month integer to corresponding month abbreviation
dun_laoghaire$month <- factor(month.abb[dun_laoghaire$month], levels = custom_order)

# Calculate monthly averages
monthly_avg <- dun_laoghaire %>%
  group_by(year, month) %>%
  summarize(avg_surge_daily_max = mean(surge_daily_max), na.rm = TRUE) %>% 
  ungroup() %>% 
  mutate(time_index = 1:n())

annual_avg <- dun_laoghaire %>%
  group_by(year) %>%
  summarize(avg_surge_daily_max = mean(surge_daily_max), na.rm = TRUE) %>% 
  ungroup() %>% 
  mutate(time_index = 1:n())


# Plot monthly averages with custom ordering
ggplot(monthly_avg, aes(x = time_index, y = avg_surge_daily_max, group = 1)) +
  geom_line() +
  labs(title = "Monthly Averages of Sea Level Surge Daily Maximum Values (Dun Laoghaire)",
       x = "Month", y = "Avg Surge Daily Max")


ggplot(annual_avg, aes(x = year, y = avg_surge_daily_max, group = 1)) +
  geom_line() +
  labs(title = "Annual Averages of Sea Level Surge Daily Maximum Values (Dun Laoghaire)",
       x = "Year", y = "Avg Surge Daily Max")
```



Two approaches to analyzing the extremes of a dataset. 
First: Divide the data into blocks and take the maximum of each block. (GEV df)
Second: Take values over a certain high threshold. (Pareto or GP df)

Which approach will we take?


For one location, find the extreme values using both threshold and block maxima methods.(GEV/GP methods) 
Estimate the parameters of the distribution using statistical techniques such as maximum likelihood estimation (MLE) or method of moments.
With the fitted model parameters, can make inferences about extreme events, such as estimating return levels (e.g., 50-year return level) or quantiles corresponding to certain probabilities of exceedance. can also use the model to predict future extreme events based on historical data.

Heatmap Generation: create a heatmap visualization of extreme values. Each coastal location can be represented by a point on the map, with the intensity of color indicating the magnitude or frequency of extreme events at that location. You can also overlay administrative boundaries or other relevant geographic features to provide context. By creating a heatmap of extreme values across multiple coastal locations in Ireland, you can identify hotspots of extreme events

Interactive Visualization: Consider creating an interactive heatmap that allows users to explore extreme values at different locations by zooming, panning, or hovering over data points.(might be cool but not useful for a thesis?) 

```{r}
library(extRemes)

# Threshold method (GEV/GP)
# Define threshold value
threshold <- quantile(dun_laoghaire$surge_daily_max, probs = 0.9) #.9 has lower neg log likelihood than 0.95
dun_laoghaire_above_threshold <- dun_laoghaire %>% filter(surge_daily_max > threshold) ### the input should be the data that is above the threshold

dun_laoghaire_annual_max <- dun_laoghaire %>% group_by(year) %>% summarise(annual_max = max(surge_daily_max))

# Fit GEV distribution to threshold exceedances
gev_fit <- fevd(dun_laoghaire_above_threshold$surge_daily_max, threshold = threshold, method = "MLE")

# Fit GEV distribution to block maxima
gev_block_fit <- fevd(dun_laoghaire_annual_max$annual_max, method = "MLE")

# Display results
print("Threshold Method (GEV/GP):")
print(summary(gev_fit))
cat("\n")

print("Block Maxima Method (GEV/GP):")
print(summary(gev_block_fit))

#AIC and BIC, neg log likelihood all much better for threshold method
```

```{r}
#what does the gev distribution look like once you've estimated parameters
plot(gev_fit) #threshold
plot(gev_block_fit) ## plot block max fit (looks better, fits inside confidence intervals)

distill(gev_fit)
distill(gev_block_fit)
plot(gev_fit, "trace")


```

```{r}
return.level(gev_fit) #Returns for threshold is 20-year: 0.85, 100-year: 1.15. This means that if the sea-level surges 1.15 metres above the mean sea level it is a 100-year event. The mean sea level is important here because if the tide is low and there is a surge of 1.27 metres, it won't have as much of an impact as if there was a high tide which would lead to flooding. In the scope of this project we can look at threshold vs block maxima and compare between locations. Then we look at clustering and see if there are locations where these things start to group together.
return.level(gev_block_fit) #Returns for block maxima is 20-year: 1.1, 100-year: 1.27.

return.level(gev_fit, do.ci = TRUE) # return level confidence intervals
return.level(gev_block_fit, do.ci = TRUE)


ci(gev_fit, type = "parameter") # parameter confidence intervals
ci(gev_block_fit, type = "parameter")

```


```{r}

station_subset <- subset(data, station_name %in% levels(data$station_name)[1:5])

# Define function to estimate return levels for a given station
estimate_return_levels <- function(station_data) {
  # Define threshold value
  threshold <- quantile(station_data$surge_daily_max, probs = 0.9)
  
  # Filter data above threshold
  station_data_above_threshold <- station_data %>% filter(surge_daily_max > threshold)
  
  # Fit GEV distribution to threshold exceedances
  gev_fit <- fevd(station_data_above_threshold$surge_daily_max, threshold = threshold, method = "MLE")
  
  # Estimate return levels
  return_levels <- return.level(gev_fit, return.period = c(20, 100), do.ci = TRUE)
  
  # Return the return levels
  return(return_levels)
}

# Create an empty dataframe to store return level estimates for all stations
return_levels_all <- data.frame(station_name = character(),
                                 return_20yr = numeric(), return_100yr = numeric(),
                                 stringsAsFactors = FALSE)

# Loop through each station in the subset
for (station_name in unique(data$station_name)) {
  # Subset data for the current station
  station_data <- data[data$station_name == station_name, ]
  
  # Estimate return levels for the current station
  return_levels <- estimate_return_levels(station_data)
  
  # Add return levels to the dataframe
  return_levels_all <- rbind(return_levels_all, data.frame(station_name = station_name,
                                                          return_20yr = return_levels[1],
                                                          return_100yr = return_levels[2]))
}

# Remove the first row (initialized with empty strings)
return_levels_all <- return_levels_all[-1, ]

# Print the dataframe
print(return_levels_all)



```

```{r}

#plot heatmap of return level estimates
#This is for the threshold method

# Select station_name, lon, and lat columns and remove duplicate rows
data_unique <- distinct(data, station_name, lon, lat)

return_levels_with_coords <- merge(return_levels_all, data_unique[, c("station_name", "lon", "lat")], by = "station_name")

# Convert return_levels_all_with_coords to sf object
return_levels_sf <- st_as_sf(return_levels_with_coords, coords = c("lon", "lat"), crs = 4326)

# Plot map of Ireland with return levels represented by colors
tm_shape(ireland) +
  tm_borders() +
  tm_shape(return_levels_sf) +
  tm_dots(col = "return_100yr", size = 1, alpha = 0.5, title = "Return Level (100yr) Threshold Method") +
  tm_style("gray")

# Plot map of Ireland with return levels represented by colors
tm_shape(ireland) +
  tm_borders() +
  tm_shape(return_levels_sf) +
  tm_dots(col = "return_20yr", size = 1, alpha = 0.5, title = "Return Level (20yr) Threshold Method") +
  tm_style("gray")

#A return level of 1 means that the exceedance sea level would need to be 1 metre above the mean still sea level in order to be considered a 1-in-20 year event.
```




```{r}
#BLOCK MAXIMA APPROACH


# Define function to estimate return levels for a given station
estimate_return_levels_block<- function(station_data_block) {
  # Fit GEV distribution to block maxima
  gev_block_fit <- fevd(station_data_block$surge_daily_max, method = "MLE")
  
  # Estimate return levels
  return_levels_block <- return.level(gev_block_fit, return.period = c(20, 100), do.ci = TRUE)
  
  # Return the return levels
  return(return_levels_block)
}

# Create an empty dataframe to store return level estimates for all stations
return_levels_all_block <- data.frame(station_name = character(),
                                 return_20yr = numeric(), return_100yr = numeric(),
                                 stringsAsFactors = FALSE)
# Loop through each station in the subset
for (station_name in unique(data$station_name)) {
  # Subset data for the current station
  station_data_block <- data[data$station_name == station_name, ]
  
  # Estimate return levels for the current station
  return_levels_block <- estimate_return_levels_block(station_data_block)
  
  # Add return levels to the dataframe
  return_levels_all_block <- rbind(return_levels_all_block, data.frame(station_name = station_name,
                                                          return_20yr = return_levels_block[1],
                                                          return_100yr = return_levels_block[2]))
}

# Remove the first row (initialized with empty strings)
return_levels_all_block <- return_levels_all_block[-1, ]

# Print the dataframe
print(return_levels_all_block)

```
```{r}
data_unique <- distinct(data, station_name, lon, lat)

return_levels_with_coords_block <- merge(return_levels_all_block, data_unique[, c("station_name", "lon", "lat")], by = "station_name")

# Convert return_levels_all_with_coords to sf object
return_levels_sf_block <- st_as_sf(return_levels_with_coords_block, coords = c("lon", "lat"), crs = 4326)

# Plot map of Ireland with return levels represented by colors
tm_shape(ireland) +
  tm_borders() +
  tm_shape(return_levels_sf_block) +
  tm_dots(col = "return_100yr", size = 1, alpha = 0.5, title = "Return Level (100yr) Using Block Maxima") +
  tm_style("gray")

# Plot map of Ireland with return levels represented by colors
tm_shape(ireland) +
  tm_borders() +
  tm_shape(return_levels_sf_block) +
  tm_dots(col = "return_20yr", size = 1, alpha = 0.5, title = "Return Level (20yr) Using Block Maxima") +
  tm_style("gray")
```

CLUSTERING ANALYSIS
```{r}
#ELBOW METHOD
# Calculate within-cluster sum of squares for different values of k
wcss <- sapply(1:10, function(k) kmeans(return_levels_with_coords$return_20yr, centers = k)$tot.withinss)

# Plot the elbow curve
plot(1:10, wcss, type = "b", pch = 19, xlab = "Number of Clusters (k)", ylab = "Within-Cluster Sum of Squares (WCSS)",
     main = "Elbow Method for Optimal k")
```

```{r}
library(cluster)

# Select relevant variables for clustering
clustering_data <- return_levels_with_coords[, c("return_20yr", "return_100yr", "lon", "lat")]

#Standardise the data
scaled_data <- scale(clustering_data)

#K-means clustering
k <- 3  # Number of clusters
kmeans_result <- kmeans(scaled_data, centers = k)

#extract cluster assignments
cluster_assignments <- kmeans_result$cluster

return_levels_with_coords$cluster <- cluster_assignments

#Convert return_levels_all_with_coords to sf object
return_levels_sf_clusters <- st_as_sf(return_levels_with_coords, coords = c("lon", "lat"), crs = 4326)

#custom colors for clusters
custom_colors <- c("white", "orange", "red")

#Plot
tm_shape(ireland) +
  tm_borders() +
  tm_shape(return_levels_sf_clusters) +
  tm_dots(col = "cluster", palette = custom_colors, size = 1, alpha = 0.5, title = "Return Level Clusters (20yr & 100yr, Threshold Method)", title.size = 0.5) + 
  tm_style("gray")
```
CLUSTERING HAS DIFFERENT RESULTS FOR EACH RUN OF THE CODE. CAN'T FIGURE OUT HOW TO ASSIGN THE COLOURS CORRECTLY.
```{r}
silhouette_scores <- silhouette(cluster_assignments, dist(clustering_data))
mean_silhouette <- mean(silhouette_scores[, "sil_width"])
mean_silhouette

#K-Means Clustering gives a 0.39 Silhouette Score on the scale -1 to 1, indicating good clustering accuracy.
```

```{r}
```
```{r}
```